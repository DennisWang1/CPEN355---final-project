{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-24T22:35:21.062500Z"
    },
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "\n",
    "# Step 1: Read data and process\n",
    "# Read image_table.csv and price_table.csv\n",
    "image_table = pd.read_csv('D:\\\\CPEN355_project\\\\Data\\\\Image_table.csv')  # Update with actual path\n",
    "price_table = pd.read_csv('D:\\\\CPEN355_project\\\\Data\\\\Price_table.csv')  # Update with actual path\n",
    "\n",
    "# Merge the tables on 'Genmodel_ID' and keep relevant columns\n",
    "merged_data = pd.merge(image_table[['Genmodel_ID', 'Image_name']],\n",
    "                       price_table[['Genmodel_ID', 'Entry_price']],  # Assuming 'Entry_price' is the column name\n",
    "                       on='Genmodel_ID')\n",
    "\n",
    "# Count the number of images per Genmodel_ID\n",
    "image_counts = merged_data['Genmodel_ID'].value_counts()\n",
    "\n",
    "# Remove Genmodel_IDs with fewer than 100 images\n",
    "valid_genmodels = image_counts[image_counts >= 300].index\n",
    "\n",
    "# Filter merged_data to only include valid Genmodel_IDs\n",
    "filtered_data = merged_data[merged_data['Genmodel_ID'].isin(valid_genmodels)]\n",
    "\n",
    "# For Genmodel_IDs with counts > 100, randomly select 100 images\n",
    "def sample_images(group):\n",
    "    if len(group) > 500:\n",
    "        return group.sample(n=500, random_state=42)\n",
    "    else:\n",
    "        return group\n",
    "\n",
    "filtered_data = filtered_data.groupby('Genmodel_ID').apply(sample_images).reset_index(drop=True)\n",
    "\n",
    "print(\"Step 1: Data processing completed. Filtered and sampled dataset.\\n\")\n",
    "\n",
    "# Step 2: Extract features from images using pre-trained VGG16\n",
    "\n",
    "# Initialize the VGG16 model, excluding the top layers\n",
    "model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "# Function to load image and extract features\n",
    "def extract_features(img_path):\n",
    "    img = load_img(img_path, target_size=(224, 224))\n",
    "    x = img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    features = model.predict(x)\n",
    "    features = features.flatten()\n",
    "    return features\n",
    "\n",
    "# Prepare lists to hold features and prices\n",
    "features_list = []\n",
    "prices_list = []\n",
    "\n",
    "# Assuming images are stored in a directory, construct the path to images\n",
    "# You may need to adjust the path accordingly\n",
    "image_directory = 'D:\\\\CPEN355_project\\\\Data\\\\DVM_noNest_test\\\\'\n",
    "\n",
    "# Process images\n",
    "for _, row in filtered_data.iterrows():\n",
    "    img_name = row['Image_name']\n",
    "    img_path = os.path.join(image_directory, img_name)\n",
    "    if os.path.exists(img_path):\n",
    "        features = extract_features(img_path)\n",
    "        features_list.append(features)\n",
    "        prices_list.append(row['Entry_price'])\n",
    "\n",
    "print(\"Step 2: Feature extraction completed. Extracted features from all images.\\n\")\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X = np.array(features_list)\n",
    "y = np.array(prices_list)\n",
    "\n",
    "# Step 3: Normalize features and split data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Step 3: Data normalization and splitting completed. Prepared training and test sets.\\n\")\n",
    "\n",
    "# Step 4: Train SVM, collect loss over iterations\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd_regressor = SGDRegressor(loss='epsilon_insensitive', epsilon=0.1, max_iter=1000, tol=1e-3, learning_rate='invscaling', eta0=0.01, random_state=42)\n",
    "\n",
    "# Fit the model and store loss over epochs\n",
    "n_epochs = 10\n",
    "train_losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    sgd_regressor.partial_fit(X_train, y_train)\n",
    "    y_pred = sgd_regressor.predict(X_train)\n",
    "    mae = mean_absolute_error(y_train, y_pred)\n",
    "    train_losses.append(mae)\n",
    "\n",
    "print(\"Step 4: Model training completed. Collected training loss over epochs.\\n\")\n",
    "\n",
    "# Step 5: Plot loss vs. iterations\n",
    "plt.figure()\n",
    "plt.plot(range(1, n_epochs+1), train_losses, marker='o')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "print(\"Step 5: Loss plot generated.\\n\")\n",
    "\n",
    "# Step 6: Evaluate model on test set\n",
    "y_pred_test = sgd_regressor.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "mae_original = mean_absolute_error(y_test, y_pred_test)\n",
    "r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Step 6: Model evaluation completed. Mean Absolute Error: {mae_original}, R^2: {r2}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d63fe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "mae_original = mean_absolute_error(y_test, y_pred_test)\n",
    "print(f\"Mean Absolute Error: {mae_original}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
