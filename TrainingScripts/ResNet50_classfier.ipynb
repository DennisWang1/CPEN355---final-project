{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Data Preprocessing\n",
    "\n",
    "# Define image directory and load CSV\n",
    "image_directory = 'D:\\\\CPEN355_project\\\\Data\\\\DVM_noNest_test'\n",
    "data = pd.read_csv('D:\\\\CPEN355_project\\\\Data\\\\Image_table.csv')\n",
    "\n",
    "# Parse Image_name to extract Maker, Year, Genmodel_ID\n",
    "def parse_image_name(image_name):\n",
    "    parts = image_name.split('$$')\n",
    "    maker = parts[0]\n",
    "    year = parts[2]\n",
    "    genmodel_id = parts[4]\n",
    "    return maker, year, genmodel_id\n",
    "\n",
    "data[['Maker', 'Year', 'Genmodel_ID']] = data['Image_name'].apply(\n",
    "    lambda x: pd.Series(parse_image_name(x)))\n",
    "\n",
    "# Count Genmodel_ID occurrences and filter\n",
    "genmodel_counts = data['Genmodel_ID'].value_counts()\n",
    "valid_genmodels = genmodel_counts[genmodel_counts > 300].index.tolist()\n",
    "filtered_data = data[data['Genmodel_ID'].isin(valid_genmodels)]\n",
    "\n",
    "# Randomly sample 200 images per Genmodel_ID\n",
    "def sample_images(df):\n",
    "    return df.sample(n=100, random_state=42)\n",
    "\n",
    "filtered_data = filtered_data.groupby('Genmodel_ID').apply(sample_images).reset_index(drop=True)\n",
    "\n",
    "# Add full image paths\n",
    "filtered_data['Image_path'] = filtered_data['Image_name'].apply(\n",
    "    lambda x: f\"{image_directory}\\\\{x}\")\n",
    "\n",
    "# Filter non-existent images\n",
    "def image_exists(path):\n",
    "    return Path(path).is_file()\n",
    "\n",
    "exists_mask = filtered_data['Image_path'].apply(image_exists)\n",
    "filtered_data = filtered_data[exists_mask].reset_index(drop=True)\n",
    "\n",
    "# Encode labels\n",
    "label_encoders = {}\n",
    "for label, name in zip(['Maker', 'Year', 'Genmodel_ID'], ['Maker', 'Year', 'Genmodel_ID']):\n",
    "    le = LabelEncoder()\n",
    "    label_encoders[name] = le\n",
    "    filtered_data[name + '_enc'] = le.fit_transform(filtered_data[name])\n",
    "\n",
    "# Prepare data\n",
    "image_paths = filtered_data['Image_path']\n",
    "maker_labels = filtered_data['Maker_enc'].values\n",
    "year_labels = filtered_data['Year_enc'].values\n",
    "genmodel_labels = filtered_data['Genmodel_ID_enc'].values\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train_paths, X_val_paths, y_train_maker, y_val_maker, y_train_year, y_val_year, y_train_genmodel, y_val_genmodel = train_test_split(\n",
    "    image_paths, maker_labels, year_labels, genmodel_labels, test_size=0.2, random_state=42, stratify=genmodel_labels)\n",
    "\n",
    "print(f\"data preprocessing done\")\n",
    "\n",
    "# 2. Dataset Preparation\n",
    "\n",
    "img_size = 224\n",
    "batch_size = 32\n",
    "\n",
    "# Function to preprocess images\n",
    "def preprocess_image(image_path, maker_label, year_label, genmodel_label):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [img_size, img_size])\n",
    "    image = image / 255.0\n",
    "    return image, {\n",
    "        'maker_output': maker_label,\n",
    "        'year_output': year_label,\n",
    "        'genmodel_output': genmodel_label\n",
    "    }\n",
    "# Create datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_paths.values, y_train_maker, y_train_year, y_train_genmodel))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val_paths.values, y_val_maker, y_val_year, y_val_genmodel))\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(f\"dataset preparation done\")\n",
    "\n",
    "# 3. Model Construction\n",
    "\n",
    "# Use pretrained ResNet50 model\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=Input(shape=(img_size, img_size, 3)))\n",
    "x = Flatten()(base_model.output)\n",
    "1\n",
    "# Add classification heads\n",
    "maker_output = Dense(len(label_encoders['Maker'].classes_), activation='softmax', name='maker_output')(x)\n",
    "year_output = Dense(len(label_encoders['Year'].classes_), activation='softmax', name='year_output')(x)\n",
    "genmodel_output = Dense(len(label_encoders['Genmodel_ID'].classes_), activation='softmax', name='genmodel_output')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=[maker_output, year_output, genmodel_output])\n",
    "\n",
    "# Freeze the base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss={'maker_output': 'sparse_categorical_crossentropy',\n",
    "                    'year_output': 'sparse_categorical_crossentropy',\n",
    "                    'genmodel_output': 'sparse_categorical_crossentropy'},\n",
    "              metrics={'maker_output': 'accuracy',\n",
    "                       'year_output': 'accuracy',\n",
    "                       'genmodel_output': 'accuracy'})\n",
    "\n",
    "print(f\"model construction done\")\n",
    "\n",
    "# 4. Model Training\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "print(f\"model training done\")\n",
    "\n",
    "# 5. Model Evaluation\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "eval_results = model.evaluate(val_dataset)\n",
    "print(eval_results)\n",
    "print(model.metrics_names)\n",
    "\n",
    "# Correctly map the evaluation results\n",
    "metrics_dict = dict(zip(model.metrics_names, eval_results))\n",
    "\n",
    "print(f\"Maker classification accuracy: {metrics_dict['maker_output_accuracy']:.2f}\")\n",
    "print(f\"Year classification accuracy: {metrics_dict['year_output_accuracy']:.2f}\")\n",
    "print(f\"Genmodel_ID classification accuracy: {metrics_dict['genmodel_output_accuracy']:.2f}\")\n",
    "print(f\"Engine type classification accuracy: {metrics_dict['engine_type_accuracy']:.2f}\")\n",
    "\n",
    "\n",
    "# 6. Compute Precision, Recall, and F1-score\n",
    "\n",
    "# Initialize lists to collect true labels and predictions\n",
    "y_true_maker = []\n",
    "y_true_year = []\n",
    "y_true_genmodel = []\n",
    "y_pred_maker = []\n",
    "y_pred_year = []\n",
    "y_pred_genmodel = []\n",
    "\n",
    "for images, labels in val_dataset:\n",
    "    maker_labels = labels['maker_output']\n",
    "    year_labels = labels['year_output']\n",
    "    genmodel_labels = labels['genmodel_output']\n",
    "    preds = model.predict(images)\n",
    "    pred_maker, pred_year, pred_genmodel = preds\n",
    "\n",
    "    # Since labels are integers, no need to use np.argmax\n",
    "    y_true_maker.extend(maker_labels.numpy())\n",
    "    y_true_year.extend(year_labels.numpy())\n",
    "    y_true_genmodel.extend(genmodel_labels.numpy())\n",
    "\n",
    "    # Predictions are still probabilities, so use np.argmax\n",
    "    y_pred_maker.extend(np.argmax(pred_maker, axis=1))\n",
    "    y_pred_year.extend(np.argmax(pred_year, axis=1))\n",
    "    y_pred_genmodel.extend(np.argmax(pred_genmodel, axis=1))\n",
    "\n",
    "# Compute classification reports\n",
    "print(\"Maker classification report:\")\n",
    "print(classification_report(y_true_maker, y_pred_maker))\n",
    "\n",
    "print(\"Year classification report:\")\n",
    "print(classification_report(y_true_year, y_pred_year))\n",
    "\n",
    "print(\"Genmodel_ID classification report:\")\n",
    "print(classification_report(y_true_genmodel, y_pred_genmodel))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
