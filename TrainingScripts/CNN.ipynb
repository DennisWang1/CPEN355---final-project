{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (Conv2D, MaxPooling2D, Flatten, Dense,\n",
    "                                     Dropout, BatchNormalization, InputLayer)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score  # Import r2_score\n",
    "from pathlib import Path  # Import Path for checking file existence\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Paths to your CSV files\n",
    "image_table = pd.read_csv('D:\\\\CPEN355_project\\\\Data\\\\Image_table.csv')\n",
    "price_table = pd.read_csv('D:\\\\CPEN355_project\\\\Data\\\\Price_table.csv')\n",
    "\n",
    "# Read Image_table.csv into a DataFrame\n",
    "df_images = image_table\n",
    "\n",
    "# Function to extract Year from Image_name\n",
    "def extract_year(image_name):\n",
    "    # Split the Image_name by '$$'\n",
    "    parts = image_name.split('$$')\n",
    "    if len(parts) > 2:\n",
    "        year = parts[2].strip()\n",
    "        return year\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply the function to create a new 'Year' column in df_images\n",
    "df_images['Year'] = df_images['Image_name'].apply(extract_year)\n",
    "\n",
    "# Read Price_table.csv into a DataFrame\n",
    "df_prices = price_table\n",
    "\n",
    "# Convert the 'Year' columns to strings to ensure consistent data types\n",
    "df_images['Year'] = df_images['Year'].astype(str)\n",
    "df_prices['Year'] = df_prices['Year'].astype(str)\n",
    "\n",
    "# Merge the DataFrames on 'Genmodel_ID' and 'Year'\n",
    "df_merged = pd.merge(df_images, df_prices, on=['Genmodel_ID', 'Year'], how='left')\n",
    "\n",
    "df_merged = df_merged.dropna(subset=['Entry_price'])\n",
    "\n",
    "# Ensure 'Entry_price' is of numeric type\n",
    "df_merged['Entry_price'] = df_merged['Entry_price'].astype(float)\n",
    "\n",
    "\n",
    "print(\"Data merged\\n\")\n",
    "\n",
    "\n",
    "#/////////////////////////Start filtering out dataset////////////////////////////////\n",
    "\n",
    "# Count the number of images per Genmodel_ID\n",
    "image_counts = df_merged['Genmodel_ID'].value_counts()\n",
    "\n",
    "# Remove Genmodel_IDs with fewer than 300 images\n",
    "valid_genmodels = image_counts[image_counts >= 300].index\n",
    "\n",
    "# Filter merged_data to only include valid Genmodel_IDs\n",
    "filtered_data = df_merged[df_merged['Genmodel_ID'].isin(valid_genmodels)]\n",
    "\n",
    "# For Genmodel_IDs with counts > 500, randomly select 500 images\n",
    "def sample_images(group):\n",
    "    if len(group) > 1000:\n",
    "        return group.sample(n=1000, random_state=42)\n",
    "    else:\n",
    "        return group\n",
    "\n",
    "filtered_data = filtered_data.groupby('Genmodel_ID').apply(sample_images).reset_index(drop=True)\n",
    "\n",
    "print(\"Data filtered\\n\")\n",
    "\n",
    "#/////////////////////////End filtering out dataset////////////////////////////////\n",
    "\n",
    "\n",
    "# Add full image paths to filtered_data\n",
    "image_directory = 'D:\\\\CPEN355_project\\\\Data\\\\DVM_noNest_test'  # Update with your image directory\n",
    "filtered_data['Image_path'] = filtered_data['Image_name'].apply(\n",
    "    lambda x: f\"{image_directory}\\\\{x}\"\n",
    ")\n",
    "\n",
    "# Check if images exist and filter out non-existent images\n",
    "def image_exists(path):\n",
    "    return Path(path).is_file()\n",
    "\n",
    "exists_mask = filtered_data['Image_path'].apply(image_exists)\n",
    "filtered_data = filtered_data[exists_mask].reset_index(drop=True)\n",
    "\n",
    "# Now define image_paths and prices\n",
    "image_paths = filtered_data['Image_path']\n",
    "prices = filtered_data['Entry_price'].values\n",
    "\n",
    "# Prepare image data and prices\n",
    "img_size = 128  # Resize images to 128*128\n",
    "batch_size = 32\n",
    "\n",
    "scaler = StandardScaler()\n",
    "prices_scaled = scaler.fit_transform(prices.reshape(-1, 1))\n",
    "\n",
    "X_train_paths, X_test_paths, y_train_scaled, y_test_scaled = train_test_split(\n",
    "    image_paths, prices_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Data split\\n\")\n",
    "\n",
    "def preprocess_image(image_path, label):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [img_size, img_size])\n",
    "    image = image / 255.0\n",
    "    return image, label\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_paths.values, y_train_scaled))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test_paths.values, y_test_scaled))\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(16, (3, 3), activation='linear', input_shape=(img_size, img_size, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Conv2D(32, (3, 3), activation='tanh'),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='leaky_relu'),\n",
    "    MaxPooling2D(3, 3),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)  # Single output for regression\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "print(\"Model compiled\\n\")\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_cnn_model3.h5', monitor='val_loss', save_best_only=False)\n",
    "\n",
    "history = model.fit(train_dataset, epochs=20,\n",
    "                    validation_data=test_dataset,\n",
    "                    callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "print(\"Finished training\\n\")\n",
    "\n",
    "loss, mae = model.evaluate(test_dataset)\n",
    "\n",
    "print(f\"Standardized MAE: {mae}\")\n",
    "\n",
    "predictions_scaled = model.predict(test_dataset)\n",
    "predictions_scaled = np.concatenate(predictions_scaled, axis=0)\n",
    "y_test_scaled_flat = np.concatenate([y for x, y in test_dataset], axis=0)\n",
    "\n",
    "predictions = scaler.inverse_transform(predictions_scaled.reshape(-1, 1))\n",
    "y_test = scaler.inverse_transform(y_test_scaled_flat.reshape(-1, 1))\n",
    "\n",
    "mae_original = mean_absolute_error(y_test, predictions)\n",
    "print(f\"Original MAE: {mae_original}\")\n",
    "print(\"Test sample number:\", len(y_test))\n",
    "\n",
    "# Compute R² score\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f\"R² score: {r2}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_images = []\n",
    "test_labels = []\n",
    "for image, label in test_dataset.unbatch().take(5):\n",
    "    test_images.append(image.numpy())\n",
    "    test_labels.append(label.numpy())\n",
    "\n",
    "test_predictions = model.predict(np.array(test_images))\n",
    "test_predictions = scaler.inverse_transform(test_predictions)\n",
    "\n",
    "for i in range(len(test_images)):\n",
    "    plt.imshow(test_images[i])\n",
    "    plt.axis('off')\n",
    "\n",
    "    actual_price = scaler.inverse_transform(test_labels[i].reshape(-1, 1))[0][0]\n",
    "    predicted_price = test_predictions[i][0]\n",
    "    plt.title(f\"Predicted price: {predicted_price:.2f}, Actual price: {actual_price:.2f}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation MAE\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['mae'], label='Train MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.title('Training and Validation MAE Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
